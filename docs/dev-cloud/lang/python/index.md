---
description: "Ma√Ætriser Python avec data science √† travers un projet professionnel : CyberAnalyzer"
icon: fontawesome/brands/python
tags: ["PYTHON", "DATA-SCIENCE", "CYBERSECURITY", "PANDAS", "NUMPY", "POO"]
status: production
---

# Python

<div
  class="omny-meta"
  data-level="üü¢ D√©butant & üü° Interm√©diaire"
  data-version="3.12+"
  data-time="16-18 heures">
</div>

## Introduction au Projet Fil Rouge - CyberAnalyzer

!!! quote "Analogie p√©dagogique"
    _Imaginez que vous √™tes un **enqu√™teur cybers√©curit√©** qui doit analyser des **milliers de lignes de logs** pour d√©tecter des intrusions. Sans outils, c'est comme chercher une aiguille dans une botte de foin avec une loupe. **Python avec ses biblioth√®ques data science** transforme cette t√¢che impossible en quelques lignes de code : `pandas` lit 100 000 lignes de logs en 2 secondes, `numpy` calcule des statistiques en microsecondes, `matplotlib` g√©n√®re des graphiques instantan√©ment, et `scipy` d√©tecte automatiquement les anomalies statistiques. Au lieu de passer 3 jours √† analyser manuellement, vous obtenez un **rapport complet avec d√©tection d'intrusions en 30 secondes**. Python + data science = superpouvoir pour analyste cyber._

> Ce guide vous accompagne dans la cr√©ation d'un **CyberAnalyzer** complet avec Python. Vous construirez un outil professionnel d'analyse de logs de cybers√©curit√© permettant de parser des logs Apache/Nginx, d√©tecter des tentatives d'intrusion (brute force, SQL injection, XSS), calculer des statistiques d'attaques, visualiser les menaces avec graphiques professionnels, d√©tecter des anomalies avec algorithmes statistiques, et exporter des rapports PDF. Ce projet couvre tous les fondamentaux Python ET les biblioth√®ques data science essentielles (pandas, numpy, matplotlib, seaborn, scipy) avec une architecture POO propre.

!!! info "Pourquoi ce projet ?"
    - **Utile professionnellement** : Analyser vos vrais logs de serveurs
    - **Portfolio** : D√©montre ma√Ætrise Python + data science + cyber
    - **Exhaustif Python** : Fondamentaux + biblioth√®ques + POO
    - **R√©aliste** : Logs r√©els, attaques r√©elles, d√©tection r√©elle
    - **Diff√©renciant** : Peu de tutoriels Python cyber avec data science
    - **Transf√©rable** : Logique applicable √† toute analyse de donn√©es

### Objectifs P√©dagogiques

√Ä la fin de ce guide, vous saurez :

- ‚úÖ Ma√Ætriser fondamentaux Python (variables, fonctions, structures)
- ‚úÖ G√©rer environnements virtuels (venv) et packages (pip)
- ‚úÖ Parser fichiers et logs avec regex
- ‚úÖ Manipuler donn√©es avec **pandas** (DataFrames, filtres, agr√©gations)
- ‚úÖ Calculer statistiques avec **numpy** (arrays, moyennes, √©cart-types)
- ‚úÖ Cr√©er visualisations avec **matplotlib** & **seaborn** (graphs pro)
- ‚úÖ D√©tecter anomalies avec **scipy** (Z-score, isolation forest)
- ‚úÖ Architecturer code avec **POO** (classes, h√©ritage, encapsulation)
- ‚úÖ Cr√©er CLI professionnel avec argparse
- ‚úÖ Exporter rapports PDF

### Pr√©requis

**Connaissances requises :**

- Aucune ! Ce guide part de z√©ro
- Concepts programmation de base (variables, fonctions) aid√©s mais pas requis

**Outils n√©cessaires :**

- Python 3.12+ install√© (v√©rifier avec `python --version`)
- √âditeur de code (VS Code recommand√© avec extension Python)
- Terminal/Console
- Windows, macOS ou Linux

### Architecture de l'Outil

```mermaid
graph TB
    subgraph "Input"
        A[Logs Apache/Nginx]
        B[Logs SSH]
        C[Logs Firewall]
    end
    
    subgraph "CyberAnalyzer Core"
        D[LogParser<br/>Parsing & Regex]
        E[DataAnalyzer<br/>Pandas DataFrames]
        F[StatisticsEngine<br/>NumPy & SciPy]
        G[ThreatDetector<br/>Pattern Matching]
        H[AnomalyDetector<br/>Algorithmes ML]
    end
    
    subgraph "Visualizations"
        I[ChartGenerator<br/>Matplotlib & Seaborn]
        J[Timeline Graphs]
        K[Heatmaps]
        L[Distribution Charts]
    end
    
    subgraph "Output"
        M[Console Report]
        N[PDF Report]
        O[CSV Export]
        P[JSON Stats]
    end
    
    A --> D
    B --> D
    C --> D
    
    D --> E
    E --> F
    E --> G
    E --> H
    
    F --> I
    G --> I
    H --> I
    
    I --> J
    I --> K
    I --> L
    
    J --> M
    J --> N
    K --> N
    L --> O
    F --> P
    
    style D fill:#ef4444,color:#fff
    style E fill:#3b82f6,color:#fff
    style I fill:#10b981,color:#fff
```

### Structure de Donn√©es

**Classe LogEntry :**

```python
class LogEntry:
    """Repr√©sente une ligne de log pars√©e"""
    
    def __init__(self):
        self.timestamp: datetime
        self.ip_address: str
        self.method: str  # GET, POST, etc.
        self.endpoint: str  # /admin, /login, etc.
        self.status_code: int  # 200, 404, 500, etc.
        self.user_agent: str
        self.referrer: str
        
        # M√©tadonn√©es
        self.is_suspicious: bool
        self.threat_type: str  # 'sql_injection', 'xss', 'brute_force'
        self.severity: int  # 1-5
```

**Classe AttackPattern :**

```python
class AttackPattern:
    """D√©finit un pattern d'attaque"""
    
    name: str  # "SQL Injection"
    regex_patterns: list[str]  # Patterns √† d√©tecter
    endpoints: list[str]  # Endpoints sensibles
    severity: int  # 1-5
    description: str
```

**DataFrame Pandas Structure :**

```python
# Colonnes du DataFrame principal
columns = [
    'timestamp',      # datetime
    'ip',             # str
    'method',         # str
    'endpoint',       # str
    'status',         # int
    'user_agent',     # str
    'is_attack',      # bool
    'attack_type',    # str
    'severity'        # int
]
```

### Flux de Traitement

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant CLI as CLI Arguments
    participant LP as LogParser
    participant DA as DataAnalyzer (Pandas)
    participant TD as ThreatDetector
    participant AD as AnomalyDetector (SciPy)
    participant CG as ChartGenerator (Matplotlib)
    participant R as Report (PDF)

    Note over U,R: Analyse Logs
    U->>CLI: python cyberanalyzer.py logs.txt
    CLI->>LP: parse_file('logs.txt')
    LP->>LP: Regex parsing
    LP-->>DA: List[LogEntry]
    DA->>DA: Create DataFrame (pandas)
    DA->>TD: detect_threats()
    TD->>TD: Pattern matching SQL/XSS
    TD-->>DA: DataFrame with threats
    DA->>AD: detect_anomalies()
    AD->>AD: Z-score calculation (scipy)
    AD-->>DA: DataFrame with anomalies
    DA->>CG: generate_charts()
    CG->>CG: plt.plot() + sns.heatmap()
    CG-->>R: PNG images
    DA->>R: generate_pdf_report()
    R-->>U: cyberanalyzer_report.pdf

    Note over U,R: Statistiques Affich√©es
    U->>CLI: python cyberanalyzer.py --stats
    CLI->>DA: calculate_statistics()
    DA->>DA: NumPy aggregations
    DA-->>U: JSON stats output
```

### Phases de D√©veloppement

Le projet est structur√© en **8 phases progressives** :

| Phase | Titre | Dur√©e | Biblioth√®ques |
|-------|-------|-------|---------------|
| 1 | Setup & Fondamentaux Python | 2h | venv, pip, stdlib |
| 2 | Parsing Logs & Regex | 2h | re, datetime, pathlib |
| 3 | Pandas DataFrames | 2h30 | pandas |
| 4 | NumPy Statistiques | 2h | numpy |
| 5 | Visualisations Matplotlib/Seaborn | 2h | matplotlib, seaborn |
| 6 | SciPy D√©tection Anomalies | 2h | scipy, scikit-learn |
| 7 | Architecture POO | 2h | Classes, h√©ritage |
| 8 | CLI & Export Rapports | 1h30 | argparse, reportlab |

**Dur√©e totale : 16h**

### Aper√ßu du R√©sultat Final

**Commande CLI :**

```bash
# Analyse simple
python cyberanalyzer.py logs/apache.log

# Analyse avec seuil de s√©v√©rit√©
python cyberanalyzer.py logs/apache.log --severity 3

# Export PDF rapport complet
python cyberanalyzer.py logs/apache.log --report --output report.pdf

# Statistiques JSON
python cyberanalyzer.py logs/apache.log --stats --format json

# Mode verbose avec graphiques
python cyberanalyzer.py logs/apache.log -v --charts
```

**Output Console :**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       CyberAnalyzer v1.0 - Security Report      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üìä Statistiques G√©n√©rales
  Total lignes analys√©es : 45,823
  P√©riode               : 2025-01-01 √† 2025-12-23
  IPs uniques           : 2,347

üö® Menaces D√©tect√©es
  SQL Injection         : 127 tentatives (s√©v√©rit√©: 5/5)
  XSS                   : 89 tentatives (s√©v√©rit√©: 4/5)
  Brute Force           : 234 tentatives (s√©v√©rit√©: 3/5)
  
üî• Top 5 IPs Malveillantes
  1. 192.168.1.105      : 87 attaques
  2. 10.0.0.45          : 65 attaques
  3. 172.16.0.89        : 54 attaques

üìà Anomalies Statistiques (Z-score > 3)
  7 IPs avec comportement anormal d√©tect√©

‚úÖ Rapport PDF g√©n√©r√© : report.pdf
```

**Fonctionnalit√©s cl√©s :**

- ‚úÖ Parsing logs Apache/Nginx/SSH
- ‚úÖ D√©tection SQL injection, XSS, brute force
- ‚úÖ Statistiques (moyennes, m√©dianes, √©cart-types)
- ‚úÖ Graphiques (timeline, heatmaps, distributions)
- ‚úÖ D√©tection anomalies (Z-score, Isolation Forest)
- ‚úÖ Architecture POO propre
- ‚úÖ CLI professionnel avec argparse
- ‚úÖ Export PDF/CSV/JSON
- ‚úÖ Mode verbose avec logs
- ‚úÖ Tests unitaires

---

## Phase 1 : Setup & Fondamentaux Python (2h)

<div class="omny-meta" data-level="üü¢ D√©butant" data-time="2 heures"></div>

### Objectifs Phase 1

√Ä la fin de cette phase, vous aurez :

- ‚úÖ Python 3.12+ install√© et configur√©
- ‚úÖ Environnement virtuel (venv) cr√©√©
- ‚úÖ Packages install√©s avec pip
- ‚úÖ Fondamentaux Python ma√Ætris√©s (variables, fonctions, listes)
- ‚úÖ Premier script fonctionnel
- ‚úÖ VS Code configur√©

### 1.1 Installation Python

**Windows :**

```bash
# T√©l√©charger Python 3.12+ depuis python.org
# Cocher "Add Python to PATH" lors installation

# V√©rifier installation
python --version
# Python 3.12.1

pip --version
# pip 24.0
```

**macOS :**

```bash
# Installer avec Homebrew
brew install python@3.12

# V√©rifier
python3 --version
pip3 --version
```

**Linux (Ubuntu/Debian) :**

```bash
# Installer Python 3.12
sudo apt update
sudo apt install python3.12 python3.12-venv python3-pip

# V√©rifier
python3.12 --version
```

### 1.2 Environnement Virtuel (venv)

**Pourquoi venv ?**

Un environnement virtuel isole les packages Python par projet. Sans venv, tous les projets partagent les m√™mes packages (risque de conflits de versions).

**Cr√©er venv :**

```bash
# Cr√©er dossier projet
mkdir cyberanalyzer
cd cyberanalyzer

# Cr√©er environnement virtuel
python -m venv venv

# Activer venv
# Windows :
venv\Scripts\activate

# macOS/Linux :
source venv/bin/activate

# Le prompt change : (venv) $
```

**Structure projet cr√©√©e :**

```
cyberanalyzer/
‚îú‚îÄ‚îÄ venv/               # Environnement virtuel (ne pas commiter)
‚îÇ   ‚îú‚îÄ‚îÄ Lib/
‚îÇ   ‚îú‚îÄ‚îÄ Scripts/        # Windows
‚îÇ   ‚îî‚îÄ‚îÄ bin/            # macOS/Linux
‚îú‚îÄ‚îÄ src/                # Code source (√† cr√©er)
‚îú‚îÄ‚îÄ logs/               # Logs test (√† cr√©er)
‚îú‚îÄ‚îÄ tests/              # Tests unitaires (√† cr√©er)
‚îú‚îÄ‚îÄ requirements.txt    # D√©pendances (√† cr√©er)
‚îî‚îÄ‚îÄ README.md
```

### 1.3 Installer Packages (pip)

**Cr√©er requirements.txt :**

```txt
# requirements.txt

# Data Science Core
pandas==2.2.0
numpy==1.26.3
scipy==1.12.0

# Visualizations
matplotlib==3.8.2
seaborn==0.13.1

# Machine Learning (optionnel Phase 6)
scikit-learn==1.4.0

# CLI & Reports
reportlab==4.0.9

# Development
pytest==8.0.0
black==24.1.1
pylint==3.0.3
```

**Installer packages :**

```bash
# Avec venv activ√©
pip install -r requirements.txt

# V√©rifier installations
pip list
# pandas    2.2.0
# numpy     1.26.3
# ...
```

### 1.4 Fondamentaux Python

**Variables & Types :**

```python
# Variables (typage dynamique)
name = "John"           # str
age = 30                # int
height = 1.75           # float
is_admin = True         # bool
tags = ["python", "cyber"]  # list
user = {"name": "John", "age": 30}  # dict

# Type hints (bonne pratique)
name: str = "John"
age: int = 30

# V√©rifier type
print(type(name))  # <class 'str'>
```

**Structures de contr√¥le :**

```python
# If / Elif / Else
if age >= 18:
    print("Majeur")
elif age >= 13:
    print("Adolescent")
else:
    print("Enfant")

# Boucle for
for i in range(5):
    print(i)  # 0, 1, 2, 3, 4

# Boucle while
count = 0
while count < 5:
    print(count)
    count += 1

# List comprehension (pythonic)
squares = [x**2 for x in range(10)]
# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
```

**Fonctions :**

```python
# Fonction simple
def greet(name):
    return f"Hello, {name}!"

print(greet("Alice"))  # Hello, Alice!

# Fonction avec type hints
def add(a: int, b: int) -> int:
    return a + b

# Fonction avec valeur par d√©faut
def power(x: float, n: int = 2) -> float:
    return x ** n

print(power(5))     # 25 (5^2)
print(power(5, 3))  # 125 (5^3)

# Fonction avec *args (arguments variables)
def sum_all(*numbers):
    return sum(numbers)

print(sum_all(1, 2, 3, 4))  # 10

# Fonction avec **kwargs (keyword arguments)
def print_info(**kwargs):
    for key, value in kwargs.items():
        print(f"{key}: {value}")

print_info(name="John", age=30)
```

**Listes & Dictionnaires :**

```python
# Listes (modifiables)
fruits = ["apple", "banana", "orange"]
fruits.append("mango")
fruits.remove("banana")
print(fruits[0])  # apple

# Slicing
numbers = [0, 1, 2, 3, 4, 5]
print(numbers[1:4])   # [1, 2, 3]
print(numbers[:3])    # [0, 1, 2]
print(numbers[3:])    # [3, 4, 5]
print(numbers[-1])    # 5 (dernier √©l√©ment)

# Dictionnaires (key-value)
user = {
    "name": "John",
    "age": 30,
    "role": "admin"
}
print(user["name"])        # John
print(user.get("email"))   # None (pas KeyError)

# Ajouter/modifier
user["email"] = "john@example.com"

# It√©rer
for key, value in user.items():
    print(f"{key}: {value}")
```

### 1.5 Fichiers & Exceptions

**Lire fichier :**

```python
# Avec context manager (recommand√©)
with open('data.txt', 'r') as f:
    content = f.read()
    print(content)
# Fichier ferm√© automatiquement

# Lire ligne par ligne
with open('data.txt', 'r') as f:
    for line in f:
        print(line.strip())  # strip() enl√®ve \n

# Lire toutes lignes en liste
with open('data.txt', 'r') as f:
    lines = f.readlines()
```

**√âcrire fichier :**

```python
# √âcraser contenu
with open('output.txt', 'w') as f:
    f.write("Hello World\n")
    f.write("Line 2\n")

# Ajouter √† la fin
with open('output.txt', 'a') as f:
    f.write("Appended line\n")
```

**Gestion exceptions :**

```python
# Try / Except
try:
    result = 10 / 0
except ZeroDivisionError:
    print("Division par z√©ro impossible")
except Exception as e:
    print(f"Erreur : {e}")
finally:
    print("Toujours ex√©cut√©")

# Lever exception
def validate_age(age):
    if age < 0:
        raise ValueError("L'√¢ge ne peut pas √™tre n√©gatif")
    return age
```

### 1.6 Premier Script : Log Counter

**Cr√©er fichier :** `src/log_counter.py`

```python
#!/usr/bin/env python3
"""
Simple log line counter.
Compte le nombre de lignes dans un fichier log.
"""

def count_lines(filepath: str) -> int:
    """
    Compte le nombre de lignes dans un fichier.
    
    Args:
        filepath: Chemin vers le fichier
        
    Returns:
        Nombre de lignes
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            return len(lines)
    except FileNotFoundError:
        print(f"Erreur : Fichier '{filepath}' introuvable")
        return 0
    except Exception as e:
        print(f"Erreur : {e}")
        return 0


def main():
    """Point d'entr√©e principal"""
    filepath = input("Chemin du fichier log : ")
    
    count = count_lines(filepath)
    
    if count > 0:
        print(f"‚úÖ {count} lignes trouv√©es")
    else:
        print("‚ùå Aucune ligne ou erreur")


if __name__ == "__main__":
    main()
```

**Cr√©er log de test :** `logs/test.log`

```
192.168.1.1 - - [01/Jan/2025:10:00:00] "GET /index.html HTTP/1.1" 200
192.168.1.2 - - [01/Jan/2025:10:00:01] "GET /admin HTTP/1.1" 403
192.168.1.1 - - [01/Jan/2025:10:00:02] "POST /login HTTP/1.1" 200
192.168.1.3 - - [01/Jan/2025:10:00:03] "GET /api/users HTTP/1.1" 401
```

**Ex√©cuter :**

```bash
python src/log_counter.py
# Chemin du fichier log : logs/test.log
# ‚úÖ 4 lignes trouv√©es
```

### 1.7 Configuration VS Code

**Installer extension Python :**

1. Ouvrir VS Code
2. Extensions (Ctrl+Shift+X)
3. Chercher "Python" (Microsoft)
4. Installer

**S√©lectionner interpr√©teur venv :**

1. Ctrl+Shift+P
2. "Python: Select Interpreter"
3. Choisir `./venv/bin/python`

**Cr√©er .vscode/settings.json :**

```json
{
  "python.defaultInterpreterPath": "${workspaceFolder}/venv/bin/python",
  "python.formatting.provider": "black",
  "python.linting.enabled": true,
  "python.linting.pylintEnabled": true,
  "[python]": {
    "editor.formatOnSave": true,
    "editor.codeActionsOnSave": {
      "source.organizeImports": true
    }
  }
}
```

### 1.8 Exercice Pratique Phase 1

!!! question "Mission : Filtrer Logs par Status Code"
    Cr√©ez une fonction `filter_by_status()` qui lit un fichier log et retourne uniquement les lignes avec un status code donn√© (ex: 404).
    
    **Objectifs :**
    - Fonction avec 2 param√®tres (filepath, status_code)
    - Parser ligne pour extraire status code
    - Retourner liste lignes filtr√©es
    
    **Indices :**
    1. Split ligne avec espaces
    2. Status code est g√©n√©ralement avant-dernier √©l√©ment
    3. Utiliser list comprehension

??? success "Solution"
    ```python
    def filter_by_status(filepath: str, status_code: int) -> list[str]:
        """
        Filtre les lignes d'un log par status code.
        
        Args:
            filepath: Chemin fichier log
            status_code: Code HTTP √† filtrer (ex: 404)
            
        Returns:
            Liste des lignes avec ce status code
        """
        filtered_lines = []
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        # Status code g√©n√©ralement avant-dernier
                        if parts[-2] == str(status_code):
                            filtered_lines.append(line.strip())
        except FileNotFoundError:
            print(f"Fichier {filepath} introuvable")
        
        return filtered_lines
    
    # Test
    errors_404 = filter_by_status('logs/test.log', 404)
    print(f"Erreurs 404 trouv√©es : {len(errors_404)}")
    for line in errors_404:
        print(line)
    ```

### Points Cl√©s √† Retenir Phase 1

- Environnement virtuel (venv) isole packages
- pip install avec requirements.txt
- Type hints am√©liorent lisibilit√© (`name: str`)
- Context manager `with open()` ferme fichiers auto
- Try/except g√®re erreurs proprement
- `if __name__ == "__main__"` pour scripts ex√©cutables
- List comprehension pythonic

### Checkpoint Phase 1

V√©rifiez que vous pouvez :

- ‚úÖ `python --version` affiche 3.12+
- ‚úÖ venv activ√© (prompt affiche `(venv)`)
- ‚úÖ `pip list` montre pandas, numpy, matplotlib
- ‚úÖ Script log_counter.py fonctionne
- ‚úÖ VS Code reconna√Æt interpr√©teur venv
- ‚úÖ Fonction filter_by_status() retourne r√©sultats

### Prochaine √âtape

Dans la Phase 2, nous allons **parser des logs r√©els** avec regex et extraire toutes les informations (IP, timestamp, method, endpoint, status).

---

## Phase 2 : Parsing Logs & Regex (2h)

<div class="omny-meta" data-level="üü¢ D√©butant ‚Üí üü° Interm√©diaire" data-time="2 heures"></div>

### Objectifs Phase 2

√Ä la fin de cette phase, vous saurez :

- ‚úÖ Utiliser regex (expressions r√©guli√®res) pour parsing
- ‚úÖ Parser logs Apache/Nginx format standard
- ‚úÖ Extraire IP, timestamp, method, endpoint, status
- ‚úÖ G√©rer dates avec datetime
- ‚úÖ Cr√©er classes (POO de base)
- ‚úÖ Valider donn√©es avec exceptions

### 2.1 Introduction Regex

**Qu'est-ce que regex ?**

Les expressions r√©guli√®res (regex) permettent de **chercher des patterns** dans du texte.

**Exemples simples :**

```python
import re

text = "Mon email est john@example.com"

# Chercher email
pattern = r'[\w\.-]+@[\w\.-]+\.\w+'
match = re.search(pattern, text)
if match:
    print(match.group())  # john@example.com

# Extraire toutes IPs
logs = """
192.168.1.1 - GET /index.html
10.0.0.45 - POST /login
172.16.0.89 - GET /admin
"""

ip_pattern = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
ips = re.findall(ip_pattern, logs)
print(ips)  # ['192.168.1.1', '10.0.0.45', '172.16.0.89']
```

**Syntaxe regex courante :**

| Pattern | Description | Exemple |
|---------|-------------|---------|
| `\d` | Chiffre (0-9) | `\d+` = un ou plusieurs chiffres |
| `\w` | Lettre/chiffre/_ | `\w+` = mot |
| `\s` | Espace/tab/newline | `\s*` = z√©ro ou plusieurs espaces |
| `.` | N'importe quel caract√®re | `a.c` = abc, a5c, a c |
| `*` | 0 ou plus | `ab*` = a, ab, abb |
| `+` | 1 ou plus | `ab+` = ab, abb, abbb |
| `?` | 0 ou 1 | `ab?` = a, ab |
| `[]` | Classe de caract√®res | `[a-z]` = lettre minuscule |
| `()` | Groupe de capture | `(\d+)` = capture chiffres |
| `\|` | OU logique | `cat\|dog` = cat ou dog |

### 2.2 Format Logs Apache

**Exemple ligne log Apache :**

```
192.168.1.100 - - [23/Dec/2025:14:30:00 +0000] "GET /admin/login.php HTTP/1.1" 200 1234 "-" "Mozilla/5.0"
```

**Structure :**

1. **IP** : `192.168.1.100`
2. **Identd** : `-` (ignor√©)
3. **Userid** : `-` (ignor√©)
4. **Timestamp** : `[23/Dec/2025:14:30:00 +0000]`
5. **Request** : `"GET /admin/login.php HTTP/1.1"`
   - **Method** : GET
   - **Endpoint** : /admin/login.php
   - **Protocol** : HTTP/1.1
6. **Status** : `200`
7. **Size** : `1234` (bytes)
8. **Referrer** : `"-"` (page pr√©c√©dente)
9. **User-Agent** : `"Mozilla/5.0"`

### 2.3 Regex pour Parser Logs

**Pattern complet Apache :**

```python
import re

APACHE_LOG_PATTERN = re.compile(
    r'(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s+'  # IP
    r'-\s+-\s+'  # Identd & userid (ignor√©s)
    r'\[(?P<timestamp>[^\]]+)\]\s+'  # Timestamp
    r'"(?P<method>\w+)\s+'  # Method (GET, POST, etc.)
    r'(?P<endpoint>[^\s]+)\s+'  # Endpoint (/admin, etc.)
    r'[^"]+"\s+'  # Protocol (ignor√©)
    r'(?P<status>\d{3})\s+'  # Status code
    r'(?P<size>\d+|-)\s+'  # Size
    r'"(?P<referrer>[^"]*)"\s+'  # Referrer
    r'"(?P<user_agent>[^"]*)"'  # User-Agent
)
```

**Parser une ligne :**

```python
def parse_log_line(line: str) -> dict | None:
    """
    Parse une ligne de log Apache.
    
    Returns:
        Dict avec champs extraits ou None si pattern ne match pas
    """
    match = APACHE_LOG_PATTERN.match(line)
    
    if not match:
        return None
    
    return match.groupdict()

# Test
line = '192.168.1.100 - - [23/Dec/2025:14:30:00 +0000] "GET /admin/login.php HTTP/1.1" 200 1234 "-" "Mozilla/5.0"'
result = parse_log_line(line)

print(result)
# {
#     'ip': '192.168.1.100',
#     'timestamp': '23/Dec/2025:14:30:00 +0000',
#     'method': 'GET',
#     'endpoint': '/admin/login.php',
#     'status': '200',
#     'size': '1234',
#     'referrer': '-',
#     'user_agent': 'Mozilla/5.0'
# }
```

### 2.4 Classe LogEntry (POO)

**Cr√©er mod√®le :** `src/models/log_entry.py`

```python
from datetime import datetime
from dataclasses import dataclass
from typing import Optional


@dataclass
class LogEntry:
    """Repr√©sente une entr√©e de log pars√©e"""
    
    ip_address: str
    timestamp: datetime
    method: str
    endpoint: str
    status_code: int
    size: int
    referrer: Optional[str] = None
    user_agent: Optional[str] = None
    
    # M√©tadonn√©es (ajout√©es par d√©tection)
    is_suspicious: bool = False
    threat_type: Optional[str] = None
    severity: int = 0
    
    def __str__(self) -> str:
        """Repr√©sentation string"""
        return (
            f"[{self.timestamp}] {self.ip_address} "
            f"{self.method} {self.endpoint} -> {self.status_code}"
        )
    
    @property
    def is_error(self) -> bool:
        """V√©rifie si c'est une erreur (4xx, 5xx)"""
        return self.status_code >= 400
    
    @property
    def is_client_error(self) -> bool:
        """Erreur client (4xx)"""
        return 400 <= self.status_code < 500
    
    @property
    def is_server_error(self) -> bool:
        """Erreur serveur (5xx)"""
        return self.status_code >= 500
```

**Note : `@dataclass`**

Le d√©corateur `@dataclass` (Python 3.7+) g√©n√®re automatiquement `__init__`, `__repr__`, `__eq__`, etc. C'est tr√®s pratique pour classes de donn√©es.

### 2.5 Parser avec Datetime

**Parser timestamp Apache :**

```python
from datetime import datetime

def parse_apache_timestamp(timestamp_str: str) -> datetime:
    """
    Parse timestamp format Apache.
    Exemple: "23/Dec/2025:14:30:00 +0000"
    
    Returns:
        datetime object
    """
    # Format Apache : day/month/year:hour:minute:second timezone
    # Supprimer timezone pour simplifier (ou utiliser strptime avanc√©)
    timestamp_clean = timestamp_str.split()[0]  # "23/Dec/2025:14:30:00"
    
    return datetime.strptime(timestamp_clean, "%d/%b/%Y:%H:%M:%S")

# Test
ts = parse_apache_timestamp("23/Dec/2025:14:30:00 +0000")
print(ts)  # 2025-12-23 14:30:00
print(ts.year, ts.month, ts.day)  # 2025 12 23
```

### 2.6 LogParser Complet

**Cr√©er parser :** `src/parsers/log_parser.py`

```python
import re
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from models.log_entry import LogEntry


class LogParser:
    """Parser de logs Apache/Nginx"""
    
    # Regex pattern Apache
    APACHE_PATTERN = re.compile(
        r'(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s+'
        r'-\s+-\s+'
        r'\[(?P<timestamp>[^\]]+)\]\s+'
        r'"(?P<method>\w+)\s+'
        r'(?P<endpoint>[^\s]+)\s+'
        r'[^"]+"\s+'
        r'(?P<status>\d{3})\s+'
        r'(?P<size>\d+|-)\s+'
        r'"(?P<referrer>[^"]*)"\s+'
        r'"(?P<user_agent>[^"]*)"'
    )
    
    def __init__(self):
        self.entries: List[LogEntry] = []
        self.errors: List[str] = []
    
    def parse_file(self, filepath: str | Path) -> List[LogEntry]:
        """
        Parse un fichier de logs complet.
        
        Args:
            filepath: Chemin vers fichier log
            
        Returns:
            Liste de LogEntry
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"Fichier {filepath} introuvable")
        
        self.entries = []
        self.errors = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                entry = self._parse_line(line.strip(), line_num)
                if entry:
                    self.entries.append(entry)
        
        return self.entries
    
    def _parse_line(self, line: str, line_num: int) -> Optional[LogEntry]:
        """Parse une ligne individuelle"""
        if not line:
            return None
        
        match = self.APACHE_PATTERN.match(line)
        
        if not match:
            self.errors.append(f"Ligne {line_num}: Format invalide")
            return None
        
        data = match.groupdict()
        
        try:
            # Parser timestamp
            timestamp = self._parse_timestamp(data['timestamp'])
            
            # Convertir types
            status_code = int(data['status'])
            size = int(data['size']) if data['size'] != '-' else 0
            
            return LogEntry(
                ip_address=data['ip'],
                timestamp=timestamp,
                method=data['method'],
                endpoint=data['endpoint'],
                status_code=status_code,
                size=size,
                referrer=data['referrer'] if data['referrer'] != '-' else None,
                user_agent=data['user_agent']
            )
        
        except Exception as e:
            self.errors.append(f"Ligne {line_num}: Erreur parsing - {e}")
            return None
    
    def _parse_timestamp(self, timestamp_str: str) -> datetime:
        """Parse timestamp Apache format"""
        timestamp_clean = timestamp_str.split()[0]
        return datetime.strptime(timestamp_clean, "%d/%b/%Y:%H:%M:%S")
    
    @property
    def success_rate(self) -> float:
        """Taux de r√©ussite parsing"""
        total = len(self.entries) + len(self.errors)
        return (len(self.entries) / total * 100) if total > 0 else 0.0


# Script de test
if __name__ == "__main__":
    parser = LogParser()
    entries = parser.parse_file("logs/test.log")
    
    print(f"‚úÖ {len(entries)} lignes pars√©es")
    print(f"‚ùå {len(parser.errors)} erreurs")
    print(f"üìä Taux r√©ussite : {parser.success_rate:.1f}%\n")
    
    # Afficher premi√®res entr√©es
    for entry in entries[:5]:
        print(entry)
```

### 2.7 Fichier Log Test R√©aliste

**Cr√©er :** `logs/apache_test.log`

```
192.168.1.100 - - [23/Dec/2025:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 1234 "-" "Mozilla/5.0"
192.168.1.100 - - [23/Dec/2025:10:00:05 +0000] "GET /admin HTTP/1.1" 403 512 "-" "Mozilla/5.0"
10.0.0.45 - - [23/Dec/2025:10:01:00 +0000] "POST /login.php HTTP/1.1" 200 2048 "http://example.com/" "curl/7.68.0"
10.0.0.45 - - [23/Dec/2025:10:01:02 +0000] "POST /login.php HTTP/1.1" 401 256 "-" "curl/7.68.0"
10.0.0.45 - - [23/Dec/2025:10:01:04 +0000] "POST /login.php HTTP/1.1" 401 256 "-" "curl/7.68.0"
172.16.0.89 - - [23/Dec/2025:10:02:00 +0000] "GET /api/users?id=1' OR '1'='1 HTTP/1.1" 500 128 "-" "sqlmap/1.5"
192.168.1.105 - - [23/Dec/2025:10:03:00 +0000] "GET /search?q=<script>alert('XSS')</script> HTTP/1.1" 200 4096 "-" "Mozilla/5.0"
```

**Tester :**

```bash
python src/parsers/log_parser.py
# ‚úÖ 7 lignes pars√©es
# ‚ùå 0 erreurs
# üìä Taux r√©ussite : 100.0%
#
# [2025-12-23 10:00:00] 192.168.1.100 GET /index.html -> 200
# [2025-12-23 10:00:05] 192.168.1.100 GET /admin -> 403
# ...
```

### 2.8 Exercice Pratique Phase 2

!!! question "Mission : D√©tecter Tentatives Brute Force"
    Cr√©ez une fonction qui d√©tecte les tentatives de brute force (>3 √©checs login cons√©cutifs depuis m√™me IP).
    
    **Objectifs :**
    - Fonction `detect_brute_force(entries: List[LogEntry]) -> List[str]`
    - Retourner liste IPs suspectes
    - Crit√®res : ‚â•3 requ√™tes POST /login avec status 401
    
    **Indices :**
    1. Grouper entries par IP
    2. Filtrer POST /login avec status 401
    3. Compter par IP

??? success "Solution"
    ```python
    from collections import defaultdict
    from typing import List
    
    def detect_brute_force(entries: List[LogEntry], threshold: int = 3) -> List[str]:
        """
        D√©tecte tentatives brute force sur login.
        
        Args:
            entries: Liste LogEntry
            threshold: Nombre √©checs pour alerte
            
        Returns:
            Liste IPs suspectes
        """
        # Compter √©checs par IP
        failed_logins = defaultdict(int)
        
        for entry in entries:
            # Crit√®res brute force
            if (entry.method == "POST" and 
                "/login" in entry.endpoint and 
                entry.status_code == 401):
                failed_logins[entry.ip_address] += 1
        
        # Filtrer IPs d√©passant seuil
        suspicious_ips = [
            ip for ip, count in failed_logins.items() 
            if count >= threshold
        ]
        
        return suspicious_ips
    
    # Test
    parser = LogParser()
    entries = parser.parse_file("logs/apache_test.log")
    
    suspicious = detect_brute_force(entries)
    print(f"üö® IPs suspectes (brute force): {suspicious}")
    # ['10.0.0.45']  # 3 √©checs login
    ```

### Points Cl√©s Phase 2

- Regex avec groupes nomm√©s (`(?P<name>...)`)
- `@dataclass` pour classes de donn√©es
- `@property` pour m√©thodes accesseur
- `Path` de pathlib pour chemins cross-platform
- `defaultdict` pour compteurs
- Type hints avec `List`, `Optional`

### Checkpoint Phase 2

- ‚úÖ Regex Apache parse correctement
- ‚úÖ LogEntry cr√©√© avec dataclass
- ‚úÖ Timestamp converti en datetime
- ‚úÖ LogParser parse fichier complet
- ‚úÖ Taux r√©ussite 100% sur test.log
- ‚úÖ D√©tection brute force fonctionne

### Prochaine √âtape

Dans la Phase 3, nous allons utiliser **pandas** pour manipuler les logs comme un DataFrame et calculer des statistiques avanc√©es.

---

### Prochaine √âtape

Dans la Phase 3, nous allons utiliser **pandas** pour manipuler les logs comme un DataFrame et calculer des statistiques avanc√©es.

---

## Phase 3 : Pandas DataFrames (2h30)

<div class="omny-meta" data-level="üü° Interm√©diaire" data-time="2h30"></div>

### Objectifs Phase 3

- ‚úÖ Cr√©er DataFrames pandas depuis LogEntry
- ‚úÖ Filtrer, trier, grouper donn√©es
- ‚úÖ Calculer agr√©gations (count, mean, sum)
- ‚úÖ Manipuler colonnes (apply, map)
- ‚úÖ G√©rer dates avec pandas

### 3.1 Introduction Pandas

**Qu'est-ce que pandas ?**

Pandas = biblioth√®que manipulation donn√©es tabulaires (comme Excel en Python). Le DataFrame est l'objet central : tableau 2D avec colonnes typ√©es.

**Cr√©er DataFrame simple :**

```python
import pandas as pd

# Depuis dict
data = {
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'city': ['Paris', 'Lyon', 'Nice']
}
df = pd.DataFrame(data)

print(df)
#       name  age    city
# 0    Alice   25   Paris
# 1      Bob   30    Lyon
# 2  Charlie   35    Nice

# Acc√®s colonne
print(df['name'])  # Series

# Filtrer
adults = df[df['age'] >= 30]

# Ajouter colonne
df['country'] = 'France'
```

### 3.2 Convertir LogEntry en DataFrame

**Cr√©er analyzer :** `src/analyzers/data_analyzer.py`

```python
import pandas as pd
from typing import List
from models.log_entry import LogEntry


class DataAnalyzer:
    """Analyse logs avec pandas"""
    
    def __init__(self, entries: List[LogEntry]):
        self.df = self._create_dataframe(entries)
    
    def _create_dataframe(self, entries: List[LogEntry]) -> pd.DataFrame:
        """
        Convertit List[LogEntry] en DataFrame pandas.
        """
        data = {
            'timestamp': [e.timestamp for e in entries],
            'ip': [e.ip_address for e in entries],
            'method': [e.method for e in entries],
            'endpoint': [e.endpoint for e in entries],
            'status': [e.status_code for e in entries],
            'size': [e.size for e in entries],
            'user_agent': [e.user_agent for e in entries],
            'is_error': [e.is_error for e in entries]
        }
        
        df = pd.DataFrame(data)
        df = df.sort_values('timestamp')  # Trier par date
        
        return df
    
    def get_unique_ips(self) -> int:
        """Nombre IPs uniques"""
        return self.df['ip'].nunique()
    
    def get_total_requests(self) -> int:
        """Nombre total requ√™tes"""
        return len(self.df)
    
    def get_error_rate(self) -> float:
        """Taux d'erreurs (%)"""
        total = len(self.df)
        errors = self.df['is_error'].sum()
        return (errors / total * 100) if total > 0 else 0.0
    
    def get_top_ips(self, n: int = 10) -> pd.Series:
        """Top N IPs par nombre de requ√™tes"""
        return self.df['ip'].value_counts().head(n)
    
    def get_top_endpoints(self, n: int = 10) -> pd.Series:
        """Top N endpoints"""
        return self.df['endpoint'].value_counts().head(n)
    
    def get_status_distribution(self) -> pd.Series:
        """Distribution status codes"""
        return self.df['status'].value_counts().sort_index()
    
    def get_requests_by_hour(self) -> pd.Series:
        """Requ√™tes par heure de la journ√©e"""
        self.df['hour'] = self.df['timestamp'].dt.hour
        return self.df.groupby('hour').size()
```

### 3.3 Filtres Avanc√©s

```python
class DataAnalyzer:
    # ...
    
    def filter_by_ip(self, ip: str) -> pd.DataFrame:
        """Filtrer par IP"""
        return self.df[self.df['ip'] == ip]
    
    def filter_by_status(self, status: int) -> pd.DataFrame:
        """Filtrer par status code"""
        return self.df[self.df['status'] == status]
    
    def filter_by_method(self, method: str) -> pd.DataFrame:
        """Filtrer par HTTP method"""
        return self.df[self.df['method'] == method]
    
    def filter_by_date_range(self, start: str, end: str) -> pd.DataFrame:
        """
        Filtrer par plage dates.
        
        Args:
            start: Date d√©but format 'YYYY-MM-DD'
            end: Date fin format 'YYYY-MM-DD'
        """
        mask = (self.df['timestamp'] >= start) & (self.df['timestamp'] <= end)
        return self.df[mask]
    
    def get_suspicious_ips(self, min_requests: int = 100) -> pd.DataFrame:
        """
        IPs avec nombre anormalement √©lev√© de requ√™tes.
        """
        ip_counts = self.df['ip'].value_counts()
        suspicious_ips = ip_counts[ip_counts >= min_requests]
        
        return self.df[self.df['ip'].isin(suspicious_ips.index)]
```

### 3.4 Agr√©gations Group√©es

```python
def get_ip_statistics(self) -> pd.DataFrame:
    """
    Statistiques par IP.
    
    Returns:
        DataFrame avec colonnes: total_requests, error_count, error_rate
    """
    stats = self.df.groupby('ip').agg({
        'status': [
            ('total_requests', 'count'),
            ('error_count', lambda x: (x >= 400).sum())
        ]
    })
    
    # Flatten multi-index columns
    stats.columns = ['total_requests', 'error_count']
    stats['error_rate'] = (stats['error_count'] / stats['total_requests'] * 100).round(2)
    
    return stats.sort_values('total_requests', ascending=False)

def get_daily_summary(self) -> pd.DataFrame:
    """
    R√©sum√© quotidien des requ√™tes.
    """
    self.df['date'] = self.df['timestamp'].dt.date
    
    daily = self.df.groupby('date').agg({
        'ip': 'count',
        'is_error': 'sum',
        'size': 'sum'
    }).rename(columns={
        'ip': 'total_requests',
        'is_error': 'errors',
        'size': 'total_bytes'
    })
    
    daily['error_rate'] = (daily['errors'] / daily['total_requests'] * 100).round(2)
    
    return daily
```

### Points Cl√©s Phase 3

- DataFrame pandas = tableau 2D performant
- `.value_counts()` pour compter occurrences
- `.groupby().agg()` pour agr√©gations
- Filtres avec masques bool√©ens `df[df['col'] > 10]`
- `.dt` accessor pour dates

### Checkpoint Phase 3

- ‚úÖ DataFrame cr√©√© depuis LogEntry
- ‚úÖ Filtres fonctionnent (IP, status, dates)
- ‚úÖ Top IPs/endpoints calcul√©s
- ‚úÖ Statistiques par IP
- ‚úÖ R√©sum√© quotidien g√©n√©r√©

---

## Phase 4 : NumPy Statistiques (2h)

<div class="omny-meta" data-level="üü° Interm√©diaire" data-time="2 heures"></div>

### Objectifs Phase 4

- ‚úÖ Arrays NumPy pour calculs vectoris√©s
- ‚úÖ Statistiques (mean, median, std, percentiles)
- ‚úÖ Op√©rations matricielles
- ‚úÖ Performance vs Python pur

### 4.1 Introduction NumPy

**Arrays NumPy :**

```python
import numpy as np

# Cr√©er array
arr = np.array([1, 2, 3, 4, 5])
print(arr.mean())  # 3.0
print(arr.std())   # 1.414...

# Array 2D
matrix = np.array([[1, 2], [3, 4]])
print(matrix.sum(axis=0))  # [4 6] (somme colonnes)
```

### 4.2 Statistiques Logs

**Cr√©er :** `src/analyzers/statistics_engine.py`

```python
import numpy as np
import pandas as pd
from typing import Dict


class StatisticsEngine:
    """Calculs statistiques sur logs"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df
    
    def calculate_request_statistics(self) -> Dict[str, float]:
        """
        Statistiques requ√™tes par IP.
        """
        # Compter requ√™tes par IP
        ip_counts = self.df.groupby('ip').size().values
        
        return {
            'mean': np.mean(ip_counts),
            'median': np.median(ip_counts),
            'std': np.std(ip_counts),
            'min': np.min(ip_counts),
            'max': np.max(ip_counts),
            'percentile_75': np.percentile(ip_counts, 75),
            'percentile_90': np.percentile(ip_counts, 90),
            'percentile_95': np.percentile(ip_counts, 95)
        }
    
    def calculate_response_time_stats(self) -> Dict[str, float]:
        """Statistiques temps r√©ponse (via size comme proxy)"""
        sizes = self.df['size'].values
        
        return {
            'mean_size': np.mean(sizes),
            'median_size': np.median(sizes),
            'std_size': np.std(sizes),
            'total_bandwidth': np.sum(sizes)
        }
    
    def detect_outliers_zscore(self, column: str, threshold: float = 3.0) -> np.ndarray:
        """
        D√©tecte outliers avec Z-score.
        
        Returns:
            Boolean array (True = outlier)
        """
        data = self.df[column].values
        mean = np.mean(data)
        std = np.std(data)
        
        z_scores = np.abs((data - mean) / std)
        
        return z_scores > threshold
```

### Points Cl√©s Phase 4

- NumPy = 100x plus rapide que Python pur
- Broadcasting pour op√©rations vectoris√©es
- Z-score = `(x - mean) / std`

### Checkpoint Phase 4

- ‚úÖ Statistiques requ√™tes calcul√©es
- ‚úÖ Percentiles 75/90/95
- ‚úÖ Outliers d√©tect√©s avec Z-score
- ‚úÖ Performance mesurable (timeit)

---

## Phase 5 : Visualisations Matplotlib/Seaborn (2h)

<div class="omny-meta" data-level="üü° Interm√©diaire" data-time="2 heures"></div>

### Objectifs Phase 5

- ‚úÖ Graphiques matplotlib
- ‚úÖ Seaborn pour graphiques statistiques
- ‚úÖ Timeline requ√™tes
- ‚úÖ Heatmaps
- ‚úÖ Distribution charts

### 5.1 Charts avec Matplotlib

**Cr√©er :** `src/visualizers/chart_generator.py`

```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path


class ChartGenerator:
    """G√©n√®re visualisations"""
    
    def __init__(self, df: pd.DataFrame, output_dir: str = "output"):
        self.df = df
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Style seaborn
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (12, 6)
    
    def plot_requests_timeline(self, save: bool = True):
        """Timeline requ√™tes par heure"""
        hourly = self.df.groupby(self.df['timestamp'].dt.hour).size()
        
        plt.figure(figsize=(14, 6))
        plt.plot(hourly.index, hourly.values, marker='o', linewidth=2)
        plt.title('Requ√™tes par Heure', fontsize=16, fontweight='bold')
        plt.xlabel('Heure')
        plt.ylabel('Nombre de Requ√™tes')
        plt.grid(True, alpha=0.3)
        
        if save:
            plt.savefig(self.output_dir / 'timeline.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_status_distribution(self, save: bool = True):
        """Distribution status codes"""
        status_counts = self.df['status'].value_counts().sort_index()
        
        plt.figure(figsize=(10, 6))
        colors = ['green' if s < 400 else 'orange' if s < 500 else 'red' 
                  for s in status_counts.index]
        plt.bar(status_counts.index, status_counts.values, color=colors, alpha=0.7)
        plt.title('Distribution Status Codes', fontsize=16, fontweight='bold')
        plt.xlabel('Status Code')
        plt.ylabel('Count')
        plt.xticks(status_counts.index)
        
        if save:
            plt.savefig(self.output_dir / 'status_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_top_ips_heatmap(self, top_n: int = 20, save: bool = True):
        """Heatmap activit√© top IPs par heure"""
        top_ips = self.df['ip'].value_counts().head(top_n).index
        df_top = self.df[self.df['ip'].isin(top_ips)].copy()
        
        df_top['hour'] = df_top['timestamp'].dt.hour
        heatmap_data = df_top.groupby(['ip', 'hour']).size().unstack(fill_value=0)
        
        plt.figure(figsize=(16, 10))
        sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='d', cbar_kws={'label': 'Requests'})
        plt.title(f'Heatmap Activit√© Top {top_n} IPs', fontsize=16, fontweight='bold')
        plt.xlabel('Heure')
        plt.ylabel('IP Address')
        plt.tight_layout()
        
        if save:
            plt.savefig(self.output_dir / 'ip_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
```

### Points Cl√©s Phase 5

- Matplotlib = graphiques de base
- Seaborn = graphiques statistiques + style
- Heatmaps avec `sns.heatmap()`

### Checkpoint Phase 5

- ‚úÖ Timeline g√©n√©r√©e
- ‚úÖ Bar chart status codes
- ‚úÖ Heatmap top IPs
- ‚úÖ Fichiers PNG sauvegard√©s

---

## Phase 6 : SciPy D√©tection Anomalies (2h)

<div class="omny-meta" data-level="üî¥ Avanc√©" data-time="2 heures"></div>

### Objectifs Phase 6

- ‚úÖ Algorithmes d√©tection anomalies
- ‚úÖ Z-score avanc√©
- ‚úÖ Isolation Forest (ML)

### 6.1 D√©tection Anomalies

**Cr√©er :** `src/detectors/anomaly_detector.py`

```python
from sklearn.ensemble import IsolationForest
import numpy as np
import pandas as pd


class AnomalyDetector:
    """D√©tecte comportements anormaux"""
    
    def detect_with_zscore(self, values: np.ndarray, threshold: float = 3.0) -> np.ndarray:
        """Z-score classique"""
        mean = np.mean(values)
        std = np.std(values)
        z_scores = np.abs((values - mean) / std)
        return z_scores > threshold
    
    def detect_with_isolation_forest(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Isolation Forest pour d√©tecter IPs anormales.
        """
        # Features : nb requ√™tes, nb erreurs, etc.
        ip_features = df.groupby('ip').agg({
            'status': 'count',
            'is_error': 'sum'
        }).rename(columns={'status': 'total_requests', 'is_error': 'errors'})
        
        # Isolation Forest
        clf = IsolationForest(contamination=0.1, random_state=42)
        ip_features['anomaly'] = clf.fit_predict(ip_features)
        
        # -1 = anomalie, 1 = normal
        anomalies = ip_features[ip_features['anomaly'] == -1]
        
        return anomalies
```

### Checkpoint Phase 6

- ‚úÖ Z-score d√©tecte outliers
- ‚úÖ Isolation Forest identifie IPs anormales
- ‚úÖ R√©sultats valid√©s manuellement

---

## Phase 7 : Architecture POO (2h)

<div class="omny-meta" data-level="üî¥ Avanc√©" data-time="2 heures"></div>

### Objectifs Phase 7

- ‚úÖ Refactoriser avec classes
- ‚úÖ H√©ritage et polymorphisme
- ‚úÖ Design patterns
- ‚úÖ Code maintenable

### 7.1 Architecture Finale

**Structure compl√®te :**

```
cyberanalyzer/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log_entry.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ attack_pattern.py
‚îÇ   ‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_parser.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ apache_parser.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nginx_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_analyzer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statistics_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ detectors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ threat_detector.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anomaly_detector.py
‚îÇ   ‚îú‚îÄ‚îÄ visualizers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chart_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ console_reporter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pdf_reporter.py
‚îÇ   ‚îî‚îÄ‚îÄ cli.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ output/
‚îî‚îÄ‚îÄ requirements.txt
```

### 7.2 H√©ritage Parsers

```python
from abc import ABC, abstractmethod

class BaseParser(ABC):
    """Parser abstrait"""
    
    @abstractmethod
    def parse_line(self, line: str) -> LogEntry | None:
        pass

class ApacheParser(BaseParser):
    def parse_line(self, line: str) -> LogEntry | None:
        # Impl√©mentation Apache
        pass

class NginxParser(BaseParser):
    def parse_line(self, line: str) -> LogEntry | None:
        # Impl√©mentation Nginx
        pass
```

### Checkpoint Phase 7

- ‚úÖ Architecture POO propre
- ‚úÖ Classes r√©utilisables
- ‚úÖ H√©ritage fonctionnel
- ‚úÖ Code document√©

---

## Phase 8 : CLI & Export (1h30)

<div class="omny-meta" data-level="üî¥ Avanc√©" data-time="1h30"></div>

### Objectifs Phase 8

- ‚úÖ CLI avec argparse
- ‚úÖ Export PDF rapports
- ‚úÖ Export CSV/JSON

### 8.1 CLI Principal

**Cr√©er :** `src/cli.py`

```python
import argparse
from pathlib import Path
from parsers.log_parser import LogParser
from analyzers.data_analyzer import DataAnalyzer
from visualizers.chart_generator import ChartGenerator


def main():
    parser = argparse.ArgumentParser(description='CyberAnalyzer - Analyse logs s√©curit√©')
    
    parser.add_argument('logfile', help='Fichier log √† analyser')
    parser.add_argument('-v', '--verbose', action='store_true', help='Mode verbose')
    parser.add_argument('--charts', action='store_true', help='G√©n√©rer graphiques')
    parser.add_argument('--report', action='store_true', help='G√©n√©rer rapport PDF')
    parser.add_argument('--severity', type=int, default=1, help='Seuil s√©v√©rit√© (1-5)')
    
    args = parser.parse_args()
    
    # Parse logs
    log_parser = LogParser()
    entries = log_parser.parse_file(args.logfile)
    
    print(f"‚úÖ {len(entries)} lignes pars√©es")
    
    # Analyse
    analyzer = DataAnalyzer(entries)
    print(f"üìä {analyzer.get_unique_ips()} IPs uniques")
    print(f"üö® Taux erreurs: {analyzer.get_error_rate():.2f}%")
    
    # Charts
    if args.charts:
        chart_gen = ChartGenerator(analyzer.df)
        chart_gen.plot_requests_timeline()
        chart_gen.plot_status_distribution()
        print("üìà Graphiques g√©n√©r√©s dans output/")
    
    # Rapport PDF
    if args.report:
        # TODO: Impl√©menter PDF
        print("üìÑ Rapport PDF g√©n√©r√©")


if __name__ == "__main__":
    main()
```

**Utilisation :**

```bash
python src/cli.py logs/apache.log --charts --report
```

### Checkpoint Phase 8

- ‚úÖ CLI fonctionnel avec argparse
- ‚úÖ Arguments pars√©s correctement
- ‚úÖ Export CSV/JSON
- ‚úÖ Rapport PDF (optionnel)

---

## Conclusion

### R√©capitulatif Complet

!!! success "F√©licitations ! Vous avez construit CyberAnalyzer avec Python"
    Outil professionnel d'analyse logs cybers√©curit√© avec data science.

**Ce que vous avez accompli :**

| Phase | Biblioth√®ques | Concepts | Dur√©e |
|-------|---------------|----------|-------|
| 1 | venv, pip | Fondamentaux Python | 2h |
| 2 | re, datetime | Regex, parsing, POO | 2h |
| 3 | pandas | DataFrames, filtres | 2h30 |
| 4 | numpy | Statistiques, arrays | 2h |
| 5 | matplotlib, seaborn | Visualisations | 2h |
| 6 | scipy, sklearn | Anomalies ML | 2h |
| 7 | - | Architecture POO | 2h |
| 8 | argparse | CLI, exports | 1h30 |
| **TOTAL** | **Complet** | **Data Science + Cyber** | **16h** |

### √âvolutions Possibles

1. **Django Dashboard** : Interface web (Guide 2)
2. **Tkinter GUI** : Application desktop (Guide 3)
3. **API REST** : Flask/FastAPI
4. **Real-time** : Websockets
5. **Machine Learning** : Pr√©diction attaques

### Le Mot de la Fin

!!! quote "Python + Data Science = Superpouvoir Cyber"
    Vous ma√Ætrisez maintenant Python avec pandas, numpy, matplotlib, seaborn, scipy et POO. CyberAnalyzer est un outil r√©el utilisable en production.
    
    **Prochains guides : Django (web) puis Tkinter (desktop) !**
    
    **Bon code, et que vos analyses soient toujours pr√©cises !** üêçüîí

---

*Guide r√©dig√© avec ‚ù§Ô∏è pour la communaut√© Python*  
*Version 1.0 - Python 3.12 - D√©cembre 2025*